# This workflow will install Python dependencies, run tests and lint with a single version of Python
# For more information see: https://docs.github.com/en/actions/automating-builds-and-tests/building-and-testing-python

name: Python application

on:
  push:
    branches: [ "main" ]
  pull_request:
    branches: [ "main" ]

permissions:
  contents: read

jobs:
  build:

    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4
    - name: Set up Python 3.10
      uses: actions/setup-python@v3
      with:Google Python SDK for the Gemini API
PyPI version Python support PyPI - Downloads

The Google AI Python SDK enables developers to use Google's state-of-the-art generative AI models (like Gemini and PaLM) to build AI-powered features and applications. This SDK supports use cases like:

Generate text from text-only input
Generate text from text-and-images input (multimodal) (for Gemini only)
Build multi-turn conversations (chat)
Embedding
For example, with just a few lines of code, you can access Gemini's multimodal capabilities to generate text from text-and-image input:

model = genai.GenerativeModel('gemini-pro-vision')

cookie_picture = {
    'mime_type': 'image/png',
    'data': Path('cookie.png').read_bytes()
}
prompt = "Give me a recipe for this:"

response = model.generate_content(
    contents=[prompt, cookie_picture]
)
print(response.text)
Install from PyPI.

pip install google-generativeai

Obtain an API key from AI Studio, then configure it here.

Import the SDK and load a model.

import google.generativeai as genai

genai.configure(api_key=os.environ["API_KEY"])

model = genai.GenerativeModel('gemini-pro')
Use GenerativeModel.generate_content to have the model complete some initial text.

response = model.generate_content("The opposite of hot is")
print(response.text)  # cold.
Use GenerativeModel.start_chat to have a discussion with a model.

chat = model.start_chat()
response = chat.send_message('Hello, what should I have for dinner?')
print(response.text) #  'Here are some suggestions...'
response = chat.send_message("How do I cook the first one?")
Run pip install google-generativeai.

For detailed instructions, you can find a quickstart for the Google AI Python SDK in the Google documentation.

This quickstart describes how to add your API key and install the SDK in your app, initialize the model, and then call the API to access the model. It also describes some additional use cases and features, like streaming, embedding, counting tokens, and controlling responses.

Find complete documentation for the Google AI SDKs and the Gemini model in the Google documentation: https://ai.google.dev/docs

See Contributing for more information on contributing to the Google AI Python SDK.

Developers who use the PaLM API

Migrate to use the Gemini API

Check our migration guide in the Google documentation.

Installation and usage for the PaLM API

Install from PyPI.

pip install google-generativeai

Obtain an API key from AI Studio, then configure it here.

import google.generativeai as palm

palm.configure(api_key=os.environ["PALM_API_KEY"])
Use palm.generate_text to have the model complete some initial text.

response = palm.generate_text(prompt="The opposite of hot is")
print(response.result)  # cold.
Use palm.chat to have a discussion with a model.

response = palm.chat(messages=["Hello."])
print(response.last) #  'Hello! What can I help you with?'
response.reply("Can you tell me a joke?")
Documentation for the PaLM API

General PaLM documentation

Text quickstart

Chat quickstart

Tuning quickstart

%pip install -q google-generativeai
%load_ext google.generativeai.notebook
Once installed, use the Python client via the %%llm Colab magic. Read the full guide here.

%%llm
The best thing since sliced bread is
The contents of this repository are licensed under the Apache License, version 2.0.
        python-version: "3.10"
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 pytest
        if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
    - name: Lint with flake8
      run:             - name: Setup Java JDK
  uses: actions/setup-java@v4.2.1
  with:
    # The Java version to set up. Takes a whole or semver Java version. See examples of supported syntax in README file
    java-version: # optional
    # The path to the `.java-version` file. See examples of supported syntax in README file
    java-version-file: # optional
    # Java distribution. See the list of supported distributions in README file
    distribution: 
    # The package type (jdk, jre, jdk+fx, jre+fx)
    java-package: # optional, default is jdk
    # The architecture of the package (defaults to the action runner's architecture)
    architecture: # optional
    # Path to where the compressed JDK is located
    jdkFile: # optional
    # Set this option if you want the action to check for the latest available version that satisfies the version spec
    check-latest: # optional
    # ID of the distributionManagement repository in the pom.xml file. Default is `github`
    server-id: # optional, default is github
    # Environment variable name for the username for authentication to the Apache Maven repository. Default is $GITHUB_ACTOR
    server-username: # optional, default is GITHUB_ACTOR
    # Environment variable name for password or token for authentication to the Apache Maven repository. Default is $GITHUB_TOKEN
    server-password: # optional, default is GITHUB_TOKEN
    # Path to where the settings.xml file will be written. Default is ~/.m2.
    settings-path: # optional
    # Overwrite the settings.xml file if it exists. Default is "true".
    overwrite-settings: # optional, default is true
    # GPG private key to import. Default is empty string.
    gpg-private-key: # optional
    # Environment variable name for the GPG private key passphrase. Default is $GPG_PASSPHRASE.
    gpg-passphrase: # optional
    # Name of the build platform to cache dependencies. It can be "maven", "gradle" or "sbt".
    cache: # optional
    # The path to a dependency file: pom.xml, build.gradle, build.sbt, etc. This option can be used with the `cache` option. If this option is omitted, the action searches for the dependency file in the entire repository. This option supports wildcards and a list of file names for caching multiple dependencies.
    cache-dependency-path: # optional
    # Workaround to pass job status to post job step. This variable is not intended for manual setting
    job-status: # optional, default is ${{ job.status }}
    # The token used to authenticate when fetching version manifests hosted on github.com, such as for the Microsoft Build of OpenJDK. When running this action on github.com, the default value is sufficient. When running on GHES, you can pass a personal access token for github.com if you are experiencing rate limiting.
    token: # optional, default is ${{ github.server_url == 'https://github.com' && github.token || '' }}
    # Name of Maven Toolchain ID if the default name of "${distribution}_${java-version}" is not wanted. See examples of supported syntax in Advanced Usage file
    mvn-toolchain-id: # optional
    # Name of Maven Toolchain Vendor if the default name of "${distribution}" is not wanted. See examples of supported syntax in Advanced Usage file
    mvn-toolchain-vendor: # optional
          
        # 
        stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    - name: Test with pytest
      run: npm publish
        # pytest
